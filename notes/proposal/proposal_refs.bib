@online{rimskySteeringLlamaContrastive2024,
  title = {Steering {{Llama}} 2 via {{Contrastive Activation Addition}}},
  author = {Rimsky, Nina and Gabrieli, Nick and Schulz, Julian and Tong, Meg and Hubinger, Evan and Turner, Alexander Matt},
  date = {2024-03-06},
  eprint = {2312.06681},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.06681},
  url = {http://arxiv.org/abs/2312.06681},
  urldate = {2024-03-19},
  abstract = {We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes "steering vectors" by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights into CAA's mechanisms by employing various activation space interpretation methods. CAA accurately steers model outputs and sheds light on how high-level concepts are represented in Large Language Models (LLMs).},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/eugen/Zotero/storage/95RWEZ4U/Rimsky et al. - 2024 - Steering Llama 2 via Contrastive Activation Addition.pdf;/Users/eugen/Zotero/storage/8QJW9IN3/2312.html}
}

@online{turnerActivationAdditionSteering2023,
  title = {Activation {{Addition}}: {{Steering Language Models Without Optimization}}},
  shorttitle = {Activation {{Addition}}},
  author = {Turner, Alexander Matt and Thiergart, Lisa and Udell, David and Leech, Gavin and Mini, Ulisse and MacDiarmid, Monte},
  date = {2023-11-13},
  eprint = {2308.10248},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.10248},
  url = {http://arxiv.org/abs/2308.10248},
  urldate = {2024-03-19},
  abstract = {Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering and guided decoding. We instead investigate activation engineering: modifying activations at inference-time to predictably alter model behavior. We bias the forward pass with a 'steering vector' implicitly specified through natural language. Past work learned these steering vectors; our Activation Addition (ActAdd) method instead computes them by taking the activation differences which result from pairs of prompts. We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet, and replicate the effect on Llama-13B and GPT-J-6B. Our approach yields inference-time control over high-level properties of output \& preserves performance on off-target topics. The method requires far less compute and implementation effort than finetuning and RLHF, allows for natural language specification by users, and its overhead scales naturally with model size.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/eugen/Zotero/storage/9GL6TE45/Turner et al. - 2023 - Activation Addition Steering Language Models Without Optimization.pdf;/Users/eugen/Zotero/storage/LQ3LGLSJ/2308.html}
}
